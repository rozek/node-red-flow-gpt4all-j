[
    {
        "id": "546a873f4d15546c",
        "type": "function",
        "z": "50ed45470ea01d88",
        "name": "GPT4All-J",
        "func": "(async () => {\n  let Prompt = (msg.payload || '').trim()\n  if (Prompt === '') {\n    msg.payload = 'empty or missing prompt'\n    node.send([null,msg])\n    return\n  }\n  \n/**** retrieve settings or provide defaults ****/\n\n  let Seed = parseInt(msg.seed,10)\n  if (isNaN(Seed)) { Seed = -1 }\n  \n  let Threads = parseInt(msg.threads,10)\n  if (isNaN(Threads)) { Threads = 4 }\n  Threads = Math.max(1,Threads)\n  Threads = Math.min(Threads,Math.max(1,os.cpus().length))\n\n  let Prediction = parseInt(msg.predict,10)\n  if (isNaN(Prediction)) { Prediction = 200 }\n  Prediction = Math.max(1,Math.min(Prediction,10000))\n\n  let topK = parseInt(msg.topk,10)\n  if (isNaN(topK)) { topK = 40 }\n  topK = Math.max(1,Math.min(topK,100))\n\n  let topP = parseFloat(msg.topp)\n  if (isNaN(topP)) { topP = 0.9 }\n  topP = Math.max(0.1,Math.min(topP,1.0))\n\n  let Temperature = parseFloat(msg.temperature)\n  if (isNaN(Temperature)) { Temperature = 0.9 }\n  Temperature = Math.max(0.0,Math.min(Temperature,1.0))\n\n  let Batches = parseInt(msg.batches,10)\n  if (isNaN(Batches)) { Batches = 8 }\n  Batches = Math.max(1,Math.min(Batches,100))\n\n  let Model = (msg.model || '').trim()\n  if (Model === '') { Model = 'ggml-gpt4all-j' }\n  Model = Model.replace(/[^\\/]*\\/+/g,'')\n  if (! Model.endsWith('.bin')) { Model += '.bin' }\n\n  Prompt = Prompt.replace(/\"/g,'\\\\\"')\n\n/**** combine all these settings into a command ****/\n\n  let Command = ( 'cd ai && ' +\n    './gpt-j --model ./' + Model + ' --n_predict ' + Prediction + \n    ' --threads ' + Threads + ' --batch_size ' + Batches +\n    ' --seed ' + Seed + ' --temp ' + Temperature +\n    ' --top_k ' + topK + ' --top_p ' + topP +\n    ' --prompt \"' + Prompt + '\"'\n  )\n\n/**** extract actual reponse from command output ****/\n\n  function ResponseFrom (Text) {\n    let HeaderLength = Text.indexOf('\\n\\n')\n    Text = Text.slice(HeaderLength + 2)\n\n    let TrailerIndex = Text.indexOf('<|endoftext|>')\n    if (TrailerIndex < 0) {\n      TrailerIndex = Text.indexOf('\\nmain: mem per token')\n    }\n    Text = Text.slice(0,TrailerIndex)\n\n    return Text\n  }\n\n/**** now infer a response from the given prompt ****/\n  \n  let { stdout,stderr, StatusCode,Signal } = child_process.spawnSync(\n    'bash', [], { input:Command }\n  )\n  \n  stdout = stdout.toString().trim()\n  stderr = stderr.toString().trim()\n\n  switch (true) {\n    case (StatusCode == null):\n    case (StatusCode === 0):\n      msg.statusCode = (stdout === '' ? 204 : 200)\n      msg.payload    = ResponseFrom(stdout)\n      break\n    default:\n      msg.statusCode = 500 + StatusCode\n      msg.payload    = (stdout === '' ? '' : '>>>> stdout >>>>\\n' + stdout + '\\n') +\n                       '>>>> stderr >>>>\\n' + stderr +\n                       (Signal == null ? '' : '\\n' + Signal)\n      break\n  }\n\n  node.send([msg,null])\n})()\n",
        "outputs": 2,
        "noerr": 0,
        "initialize": "",
        "finalize": "",
        "libs": [
            {
                "var": "os",
                "module": "os"
            },
            {
                "var": "child_process",
                "module": "child_process"
            }
        ],
        "x": 310,
        "y": 680,
        "wires": [
            [
                "a7c5b8dd9e5aeab8",
                "1f3276a0202b13bb"
            ],
            [
                "4767fc3929323699"
            ]
        ]
    }
]